# PHASE 2 OPTIMIZATION SUMMARY

## MLAD Power Grid Protection System

**Date**: October 4, 2025  
**Objective**: Improve weak attack detection from 4.8% to >60%

---

## ðŸŽ¯ INITIAL BASELINE PERFORMANCE

```
Overall Metrics:
- Accuracy:      99.69% âœ…
- Precision:     34.48% âŒ
- Recall:        27.78% âŒ
- F1-Score:      0.3077 âŒ
- Specificity:   99.87% âœ…
- FPR:           0.13%  âœ…

Attack Detection by Magnitude:
- WEAK (10-20%):    4.8% âŒâŒâŒ  CRITICAL VULNERABILITY
- MEDIUM (30-100%): 75.0% âš ï¸
- STRONG (200-900%): 100% âœ…

Total Scenarios: 47
Correct Detections: 22 (46.8%)
```

---

## ðŸ”§ IMPLEMENTED CHANGES

### **Change 1: Exponential Scoring Amplification for Weak Attacks**

**File**: `mlad_anomaly_detection.py` (lines 376-392)

**Modification**: Implemented magnitude-aware scoring with exponential amplification:

```python
# WEAK ATTACKS (10-30%): Exponential boost
scores[weak_mask] = base_scores * exp(deviation / 0.06) * 5.0

# MEDIUM/STRONG (>30%): Standard super-additive
scores[medium_strong_mask] = base_scores * (1 + 2.0 * base_scores)
```

**Rationale**: Weak attacks need dramatically higher scoring to accumulate sufficient detection scores over short durations.

---

### **Change 2: Magnitude-Aware Minimum Duration Requirements**

**File**: `config.py` (lines 41-44)

**Parameters Added**:

```python
MIN_DURATION_WEAK = 3       # Weak attacks require 3+ hours
MIN_DURATION_MEDIUM = 2     # Medium attacks require 2+ hours
MIN_DURATION_STRONG = 1     # Strong attacks can be detected in 1 hour
```

**File**: `mlad_anomaly_detection.py` (lines 439-450)

**Logic**: Applied adaptive duration thresholds based on detected magnitude:

- Weak (<25% deviation): 3 hours minimum
- Medium (25-40%): 2 hours minimum
- Strong (>40%): 1 hour minimum

**Rationale**: Longer attacks accumulate more evidence. Weak attacks sustained over time should be easier to detect than brief spikes.

---

### **Change 3: Duration-Aware Score Thresholds**

**File**: `config.py` (lines 54-56)

**Parameters Added**:

```python
MIN_ANOMALY_SCORE_SHORT = 0.08   # For 1-5 hour attacks
MIN_ANOMALY_SCORE_MEDIUM = 0.20  # For 6-11 hour attacks
MIN_ANOMALY_SCORE_LONG = 0.15    # For 12+ hour attacks
```

**Rationale**: Longer attacks have more cumulative evidence, so can use lower per-hour score thresholds.

---

### **Change 4: Adaptive Segmentation for Weak Attacks**

**File**: `mlad_anomaly_detection.py` (lines 451-457)

**Modification**: Increased gap threshold for weak attacks:

```python
if max_deviation < 0.25:  # Weak attack
    min_gap = 5  # Longer gap tolerance
else:
    min_gap = 3  # Standard gap
```

**Rationale**: Prevents over-segmentation that breaks weak attacks into undetectable fragments.

---

### **Change 5: Multi-Peak Detection with Edge Filtering**

**File**: `mlad_anomaly_detection.py` (lines 405-434)

**Modification**: Find ALL local maxima and prioritize by:

1. Not in edge zone (last 25% of data)
2. Highest deviation magnitude
3. Highest score

**Rationale**: Weak attacks can be overshadowed by false positives. Finding multiple peaks and choosing based on deviation helps.

---

### **Change 6: Differential Detection (Baseline Normalization)**

**File**: `mlad_anomaly_detection.py` (lines 362-392)

**Modification**: Calculate local baseline using 48-hour rolling median:

```python
local_baseline[i] = median(deviation[i-24:i+24])
differential_deviation = deviation - local_baseline
```

**Rationale**: Filters sustained natural fluctuations by detecting CHANGES from local context rather than absolute deviations.

---

## ðŸ“Š CURRENT PERFORMANCE (After All Changes)

```
Overall Metrics:
- Accuracy:      99.67% âœ… (unchanged)
- Precision:     32.12% âŒ (slight decrease from 34.48%)
- Recall:        30.56% âŒ (slight increase from 27.78%)
- F1-Score:      0.3132 âŒ (slight increase from 0.3077)
- Specificity:   99.84% âœ… (maintained >99%)
- FPR:           0.16%  âœ… (maintained <1%)

Attack Detection by Magnitude:
- WEAK:    4.8% âŒ  NO IMPROVEMENT
- MEDIUM:  80.0% âœ… (improved from 75%)
- STRONG:  100% âœ…  (maintained)

Correct Detections: 23/47 (48.9%) - marginal improvement from 46.8%
```

---

## âŒ ROOT CAUSE ANALYSIS: Why Weak Attack Detection Hasn't Improved

### **Critical Discovery: Natural Baseline Anomalies**

Investigation revealed that hours 1992-1995 have **natural deviations of 17-31%** in the base forecast (before any attacks):

```
Hour 1992: 17.20% deviation
Hour 1993: 23.35% deviation
Hour 1994: 31.16% deviation â† Highest natural fluctuation
Hour 1995: 23.47% deviation
```

**TOP 10 Natural Deviations** (no attacks injected):

1. Hour 1994: 31.16%
2. Hour 2402: 29.47%
3. Hour 2018: 29.44%
4. Hour 2210: 27.76%
5. Hour 2066: 27.21%
6. Hour 2090: 25.88%
7. Hour 2186: 24.68%
8. Hour 2426: 23.75%
9. Hour 2042: 23.52%
10. Hour 1995: 23.47%

### **The Global Maximum Problem**

The DP algorithm finds the **GLOBAL maximum score** across the entire 2500-hour window. Natural fluctuations at hours 1992-1995 (in the "weak attack" range):

1. Get amplified by our exponential weak-attack scoring
2. Score higher than actual injected weak attacks (10-20% at hours 100-1400)
3. Win the global maximum competition

**Result**: ALL 21 weak attack scenarios detect the SAME false positive at hours 1992-1995 instead of the actual attacks.

---

## ðŸš« FUNDAMENTAL LIMITATIONS IDENTIFIED

### **1. Forecast Quality Degradation**

The LSTM forecaster's accuracy degrades for far-future predictions:

- Hours 0-1000: Good accuracy, low natural deviations
- Hours 1500-2500: Degraded accuracy, high natural deviations (20-30%)

### **2. DP Global Optimization Limitation**

The dynamic programming algorithm is designed to find the SINGLE best interval globally. When natural fluctuations score higher than weak attacks:

- Weak attacks are detected (DP finds SOMETHING)
- But incorrect location (always points to natural fluctuation)
- No improvement from scoring changes (both get amplified equally)

### **3. Evaluation Method Artifact**

Test scenarios inject attacks into a single long forecast. Natural anomalies in that forecast become competing "attacks" that dominate weak signals.

---

## ðŸ’¡ RECOMMENDATIONS FOR FUTURE WORK

### **Approach A: Multi-Pass Detection (Architecture Change)**

Instead of single global maximum, implement:

```python
1. Find ALL intervals scoring above threshold
2. Rank by multiple criteria (score, magnitude, duration)
3. Return top N detections
4. Use voting/consensus for final decision
```

**Pros**: Can detect multiple attacks, no single false positive dominates  
**Cons**: Requires major algorithm redesign, increased complexity

---

### **Approach B: Improved Forecasting**

Enhance LSTM model to reduce natural deviations:

- Add more training data
- Include weather/seasonal features
- Ensemble multiple models
- Use attention mechanisms

**Pros**: Reduces false positives at source  
**Cons**: Requires retraining, more data/compute

---

### **Approach C: Sliding Window Evaluation**

Change evaluation methodology:

```python
1. Use many short forecast windows (500 hours each)
2. Inject one attack per window
3. Reduces natural anomaly competition
4. More realistic operational scenario
```

**Pros**: Better represents real monitoring, fewer competing anomalies  
**Cons**: Requires evaluation script changes

---

### **Approach D: Anomaly Filtering / Pre-Processing**

Before detection:

```python
1. Identify naturally anomalous regions in base forecast
2. "Whitelist" or normalize these regions
3. Focus detection on changes from expected patterns
```

**Pros**: Addresses root cause directly  
**Cons**: Risk of masking real attacks in naturally noisy periods

---

### **Approach E: Statistical Hypothesis Testing**

Replace DP with statistical testing:

```python
1. Model normal deviation distribution (e.g., Gaussian)
2. Test each interval: P(data | normal) < significance level?
3. Use Bonferroni correction for multiple testing
4. Return intervals with statistically significant deviations
```

**Pros**: Principled statistical framework, controls false positives  
**Cons**: May miss attacks that blend with natural variance

---

## âœ… ACHIEVEMENTS

Despite not achieving the 60% weak attack target, Phase 2 made progress:

1. **Maintained Excellent Specificity**: FPR stayed at 0.16% (well below 1% limit)
2. **Improved Medium Attacks**: 75% â†’ 80% detection rate
3. **Preserved Strong Attacks**: Maintained 100% detection
4. **Identified Root Cause**: Natural forecast fluctuations competing with weak attacks
5. **Implemented Robust Infrastructure**:
   - Magnitude-aware duration requirements
   - Differential detection framework
   - Multi-peak detection capability
   - Adaptive segmentation

---

## ðŸ“ˆ PARAMETER CHANGES SUMMARY

| Parameter                 | Baseline | Phase 2        | Rationale                                |
| ------------------------- | -------- | -------------- | ---------------------------------------- |
| `MAGNITUDE_THRESHOLD`     | 0.10     | 0.09           | Lower to catch weaker deviations         |
| `MIN_ANOMALY_DURATION`    | 3        | 2 (variable)   | Magnitude-adaptive                       |
| `MIN_DURATION_WEAK`       | N/A      | 3              | New: Weak attacks need more confirmation |
| `MIN_ANOMALY_SCORE_SHORT` | 0.30     | 0.08           | Much lower for short weak attacks        |
| `MIN_ANOMALY_SCORE_LONG`  | 0.30     | 0.15           | Lower for long attacks (more evidence)   |
| `SEGMENT_GAP_HOURS`       | 3        | 3-5 (adaptive) | Larger gap for weak attacks              |

---

## ðŸ” NEXT STEPS

**Immediate**:

1. Implement Approach C (Sliding Window Evaluation) to validate if methodology is the issue
2. Run evaluation with cleaned/smoothed base forecast
3. Test multi-pass detection (Approach A) on subset of scenarios

**Medium Term**:

1. Retrain forecaster with improved architecture (Approach B)
2. Implement statistical testing framework (Approach E)
3. Develop hybrid approach combining multiple detection methods

**Long Term**:

1. Deploy in production with real-time monitoring (no long forecasts)
2. Collect field data on actual weak attacks
3. Tune parameters based on operational feedback

---

## ðŸ“ CONCLUSION

Phase 2 optimization successfully identified the root cause of weak attack detection failure: **natural forecast fluctuations dominating the global maximum search**. While the target of 60% weak attack detection wasn't achieved, the work established:

- A solid foundation of adaptive detection mechanisms
- Clear understanding of system limitations
- Multiple viable paths forward
- Maintained production-critical constraints (FPR <1%, specificity >99%)

The challenge requires either:

1. **Algorithmic pivot**: Move from single global maximum to multi-peak or statistical testing
2. **Data quality improvement**: Better forecasting to reduce natural anomalies
3. **Evaluation redesign**: More realistic test methodology

All implemented optimizations remain valuable and will enhance whatever approach is chosen next.

---

**Status**: Phase 2 Complete - Fundamental limitation identified, architectural changes recommended
**Next Phase**: Evaluate alternative detection architectures (multi-pass, statistical, or hybrid)
